{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ORIGINAL DATA SET CAN BE FOUND HERE:**\n",
    "\n",
    "[Kaggle Fake News Dataset](https://www.kaggle.com/mrisdal/fake-news/downloads/fake-news.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipfile_path = '/home/farhaan/Downloads/fake-news.zip'\n",
    "df_fakenews = pd.read_csv(zipfile_path,compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fakenews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_fakenews.loc[:,'text'].sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fakenews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fakenews['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lengths = df_fakenews.text.str.len()\n",
    "len(str_lengths[str_lengths>1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(str_lengths[str_lengths>1000],bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered fakenews dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews_df_filtered = df_fakenews[pd.notnull(df_fakenews['text'])]\n",
    "fakenews_df_filtered = fakenews_df_filtered[fakenews_df_filtered['language']=='english']\n",
    "delregex1= re.compile('(?:Be the First to Comment.*articles *$|Subscribe *$|\\nRelated:.*$| Share:.*$)',re.IGNORECASE|re.DOTALL)\n",
    "delregex2 = re.compile('(?:Submit your review|Follow on (?:Facebook|Twitter)|Print this post|Share on (?:Facebook|Twitter))',re.IGNORECASE)\n",
    "delregex3 = re.compile('(?:Century Wire|Newsinfo|Delivered by The Daily Sheeple\\nWe .*$|barracuda brigade)',re.IGNORECASE)\n",
    "delregex4 = re.compile('READ MORE ELECTION NEWS AT .*$',re.IGNORECASE,re.DOTALL)\n",
    "delregex5 = re.compile('^.{0,300}Leave a Comment |Leave a Reply Click here to get more.*',re.IGNORECASE|re.DOTALL)\n",
    "delregex6 = re.compile('(?:SUBSCRIBE on YouTube|[\\n\\.].*?like and share .*|^Share on Twitter.*\\.|This article was written by .*?\\.)',re.IGNORECASE)\n",
    "# delregex7 = re.compile('\\nRelated:.*$',re.IGNORECASE|re.DOTALL)\n",
    "# delregex8 = re.compile('Subscribe *$',re.IGNORECASE)\n",
    "for ind,txt in zip(fakenews_df_filtered.index,fakenews_df_filtered.text):\n",
    "    txt = re.sub(delregex1,\"\",txt)\n",
    "    txt = re.sub(delregex2,\"\",txt)\n",
    "    txt = re.sub(delregex3,\"\",txt)\n",
    "    txt = re.sub(delregex4,\"\",txt)\n",
    "    txt = re.sub(delregex5,\"\",txt)\n",
    "    txt = re.sub(delregex6,\"\",txt)\n",
    "    \n",
    "\n",
    "#     txt = re.sub(delregex7,\"\",txt)\n",
    "#     txt = re.sub(delregex8,\"\",txt)\n",
    "    fakenews_df_filtered.set_value(ind,'text',txt)\n",
    "fakenews_df_filtered = fakenews_df_filtered[fakenews_df_filtered.text.str.len()>850]\n",
    "text= fakenews_df_filtered.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fakenews_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = 30\n",
    "index1 = fakenews_df_filtered.sample(random_state=state).index\n",
    "print(list(fakenews_df_filtered.sample(random_state=state).index),'\\n',list(fakenews_df_filtered.sample(random_state=state).text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df_fakenews.iloc[index1,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer and PCA, straight up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='ascii',analyzer='word',lowercase=True,stop_words='english',min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_names_dirty = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex='[^\\d_]*'\n",
    "feat_names_clean= [re.findall(regex,s)[0] for s in feat_names_dirty if (re.findall(regex,s)!=[] and len(re.findall(regex,s)[0])>=4 and len(re.findall(regex,s)[0])<20)]\n",
    "\n",
    "feat_names_clean= list(set(feat_names_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [w for w in feat_names_dirty if len(w)>=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([len(i) for i in feat_names_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(feat_names_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feat_names_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=feat_names_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_frequency = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u,s,vt = svds(wd_frequency.asfptype(),k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cumul=0\n",
    "n_components=[]\n",
    "cum_vals = []\n",
    "for ind,singv in enumerate(sorted(s,reverse=True)):\n",
    "    cumul += singv\n",
    "    n_components.append(ind)\n",
    "    cum_vals.append(cumul/sum(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_components,cum_vals,'g-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=300,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd.fit(wd_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cumul=0\n",
    "n_components=[]\n",
    "cum_vals = []\n",
    "for ind,singv in enumerate(tsvd.explained_variance_ratio_):\n",
    "    cumul += singv\n",
    "    n_components.append(ind+1)\n",
    "    cum_vals.append(cumul)\n",
    "    \n",
    "plt.plot(n_components,cum_vals,'g-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_wd_frequency = pd.DataFrame(tsvd.transform(wd_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanscl = KMeans(n_clusters=10,n_jobs=-1,random_state=40)\n",
    "\n",
    "kmeanscl.fit(transformed_wd_frequency)\n",
    "\n",
    "centroids = kmeanscl.cluster_centers_\n",
    "\n",
    "Counter(kmeanscl.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kmeanscl.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustered_text = pd.merge(pd.DataFrame(fakenews_df_filtered),\n",
    "                          pd.DataFrame(kmeanscl.labels_,\n",
    "                                       index=fakenews_df_filtered.index,\n",
    "                                       columns=['cluster_label']),\n",
    "                          how='left',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(clustered_text[clustered_text['cluster_label']==4]['text'].sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_text[clustered_text['language']!='english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_text[clustered_text['cluster_label']==8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list(print(doc,'\\n\\n') for doc in clustered_text[clustered_text['cluster_label']==2].sample(5).text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='ascii',analyzer='word',lowercase=True,stop_words='english',min_df=10,vocabulary=feat_names_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_frequency = vectorizer.transform(text)\n",
    "tsvd = TruncatedSVD(n_components=300,random_state=40)\n",
    "tsvd.fit(wd_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumul=0\n",
    "n_components=[]\n",
    "cum_vals = []\n",
    "for ind,singv in enumerate(tsvd.explained_variance_ratio_):\n",
    "    cumul += singv\n",
    "    n_components.append(ind+1)\n",
    "    cum_vals.append(cumul)\n",
    "    \n",
    "plt.plot(n_components,cum_vals,'g-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(ind,val) for ind,val in enumerate(tsvd.components_[0])],key=lambda x: x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = [10187,10824,2465,10654,3326,11731]\n",
    "[feat_names_clean[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_wd_frequency = pd.DataFrame(tsvd.transform(wd_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanscl = KMeans(n_clusters=10,n_jobs=-1,random_state=40)\n",
    "\n",
    "kmeanscl.fit(transformed_wd_frequency)\n",
    "\n",
    "centroids = kmeanscl.cluster_centers_\n",
    "\n",
    "Counter(kmeanscl.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(ind,val) for ind,val in enumerate(centroids[9])],key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to get the most important topics in the cluster:\n",
    "for cluster_ind in range(len(centroids)):\n",
    "    main_topics_in_cluster = [topic[0] for topic in sorted([(ind,val) for ind,val in enumerate(centroids[cluster_ind])],key=lambda x: x[1],reverse=True)[:3]]\n",
    "    topic_relative_importance = [topic[1] for topic in sorted([(ind,val) for ind,val in enumerate(centroids[cluster_ind])],key=lambda x: x[1],reverse=True)[:3]]\n",
    "    print(f\"Cluster # {cluster_ind}\",main_topics_in_cluster)\n",
    "    print(f\"Topic rel. importance: {np.around(topic_relative_importance,3)}\\n\")\n",
    "    for topic in main_topics_in_cluster:\n",
    "        words_in_topic = [word_ind[0] for word_ind in sorted([(ind,val) for ind,val in enumerate(tsvd.components_[topic])],key=lambda x: x[1],reverse=True)[:15]]\n",
    "        print(f\"Topic # {topic}\",[feat_names_clean[word_ind] for word_ind in words_in_topic])\n",
    "    excerpts = [clustered_text[clustered_text['cluster_label']==cluster_ind].loc[:,'text'].iloc[i][200:600] for i in range(5,8)]\n",
    "    for ind,excerpt in enumerate(excerpts):\n",
    "        print(f\"\\n*******Excerpt#{ind+1}: *********\\n {excerpt}\")\n",
    "    print('\\n\\n')\n",
    "    print(\"___________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustered_text = pd.merge(pd.DataFrame(fakenews_df_filtered),\n",
    "                          pd.DataFrame(kmeanscl.labels_,\n",
    "                                       index=fakenews_df_filtered.index,\n",
    "                                       columns=['cluster_label']),\n",
    "                          how='left',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(clustered_text[clustered_text['cluster_label']==9].loc[:,'text'].iloc[i][300:1000] for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_text = deepcopy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText(mod_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_docs = [doc for doc in nlp.pipe(mod_text,n_threads=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_idxs = [doc_idx for doc_idx in text.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [nlp(doc) for doc in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttxt = clustered_text[clustered_text['cluster_label']==4].text[10337]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in documents[3]:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stw = list(stopwords.words('english'))\n",
    "stw +=['.',',','!','?','\"',\"'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom stoplist\n",
    "content_specific_stopwords = ['trump', 'clinton', 'hillary', 'election', 'people', 'said', 'president', 'obama', 'campaign']\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\",\"’s\",\"'ve\"] + list(ENGLISH_STOP_WORDS)+content_specific_stopwords)\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\",'--','-', \"...\", \"“\", \"”\",'…' ,'’','–']\n",
    "\n",
    "# Every step in a pipeline needs to be a \"transformer\". \n",
    "# Define a custom transformer to clean text using spaCy\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert text to cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    \n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # split up words comlicated by some symbols\n",
    "    text = text.replace('’',\" \")\n",
    "    \n",
    "    #convert text to unicode\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"\", text)\n",
    "    \n",
    "    # replace HTML symbols\n",
    "    bracs_regex = re.compile('<.*?>')\n",
    "    text = re.sub(bracs_regex,\" \",text)\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # delete urls\n",
    "    url_reg = re.compile('\\w+(?:\\.\\w+)+')\n",
    "    text = re.sub(url_reg, \"\", text)\n",
    "    url_reg2 = re.compile('http.* ')\n",
    "    text = re.sub(url_reg2, \" \", text)\n",
    "    \n",
    "    # delete words countaining numbers\n",
    "    num_reg = re.compile('\\w*\\d+\\w*')\n",
    "    text = re.sub(num_reg, \"\", text)\n",
    "        \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # only return alphabetical words\n",
    "    letter_reg = re.compile(' .*[^a-z]+?.* ')\n",
    "    text = re.sub(letter_reg,' ',text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = nlp(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    tokens = [token for token in tokens if len(token)>=3]\n",
    "    return tokens\n",
    "\n",
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "    \n",
    "# the vectorizer and classifer to use\n",
    "# note that I changed the tokenizer in CountVectorizer to use a custom function using spaCy's tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf =  KMeans(n_clusters=10,n_jobs=-1,random_state=40)\n",
    "# the pipeline to clean, tokenize, vectorize, and classify\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl_text_tf = CleanTextTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_text_tf.fit(mod_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_txt = cl_text_tf.transform(mod_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(cl_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparsed_txt = vectorizer.transform(cl_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sparsed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=80,random_state=40)\n",
    "tsvd.fit(sparsed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sorted(tsvd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeanscl = KMeans(n_clusters=8,n_jobs=-1,random_state=40)\n",
    "\n",
    "transformed_wd_frequency = pd.DataFrame(tsvd.transform(sparsed_txt))\n",
    "\n",
    "kmeanscl.fit(transformed_wd_frequency)\n",
    "\n",
    "centroids = kmeanscl.cluster_centers_\n",
    "\n",
    "Counter(kmeanscl.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "train = mod_text\n",
    "\n",
    "# train\n",
    "pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in list(documents[0].sents)[:10]:\n",
    "    for token in sent:\n",
    "        print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtoklist = cleanText(text[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(testtoklist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst2=deepcopy(text[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('\\w+(?:\\.\\w+)+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(regex,tst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_text = deepcopy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
